{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ddb36a",
   "metadata": {},
   "source": [
    "# Détection des faux billets avec Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22a378",
   "metadata": {},
   "source": [
    "## 1. Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbce98",
   "metadata": {},
   "source": [
    "Contexte : \n",
    "\n",
    "* Identification des contrefaçons des billets en euros\n",
    "* Les billets d'euro ont des valeurs nominales de 5, 10, 20, 50, 100, 200 et 500 euros. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8419f9",
   "metadata": {},
   "source": [
    "# 1.2 Importation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5601866a",
   "metadata": {},
   "source": [
    "## 1.2.1 Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ad7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#builtin\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#visualisation \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20adb496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stat\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175f558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine learning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad0f362",
   "metadata": {},
   "source": [
    "## 2.1 chargement des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"../data/source/\"\n",
    "# Read CSV train data file into DataFrame\n",
    "train_df= pd.read_csv(os.path.join(input_folder, \"billets.csv\"), dtype=float, sep=';')\n",
    "# Read CSV test data file into DataFrame\n",
    "test_df = pd.read_csv(os.path.join(input_folder, \"billets_production.csv\"), sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefccca5",
   "metadata": {},
   "source": [
    "## 2. Prétraitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f349d6",
   "metadata": {},
   "source": [
    "## 2.2 Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage des 5 premieres lignes\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b02a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Le nombre des itemes dans le DataFrame train_df est {}.'.format(train_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview test data\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995c5c5",
   "metadata": {},
   "source": [
    "test DataFrame contient 5 lignes sans valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Le nombre des itemes dans le DataFrame train_df est {}.'.format(test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17c28d",
   "metadata": {},
   "source": [
    "Note: On ne voit pas la colonne 'is_genuine' (la target) dans le dataset test_df. Notre objectif est alors de prédire la target par differentes algorithmes de machine learning comme la regression logistique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fccada",
   "metadata": {},
   "source": [
    "### Transform is_geniune into is_fake (revserse 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d542ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_fake'] = 1 - train_df['is_genuine']\n",
    "train_df['is_fake'].astype(int)\n",
    "train_df.drop(columns=['is_genuine'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fad873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage des 5 dernieres lignes\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345ff61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Affichage de 5 lignes arbitrairement\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensions du DataFrame\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a225f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Information sur les colonnes\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistiques descriptives\n",
    "train_df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nombre des doublons dans le DataFrame\n",
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc65dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nombre des doublons sans le target\n",
    "train_df.drop(columns=\"is_fake\").duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information sur les valeurs unique dans chaque colonne\n",
    "train_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nombre des valeurs dans la target\n",
    "train_df['is_fake'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68433a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation de la colonne target\n",
    "train_df.rename(columns={'is_fake':'target'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9245cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=train_df['target'].value_counts()\n",
    "labels=data.index\n",
    "plt.pie(data, labels=labels, autopct='%1.1f%%')\n",
    "plt.title('Nombre des vrais et faux billets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e89cb",
   "metadata": {},
   "source": [
    "1=faux\n",
    "0=vrai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f930728",
   "metadata": {},
   "source": [
    "## 2.3 Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f83fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nombre des valeurs manquantes dans chaque colonne\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nombre des valeurs manquantes dans chaque colonne\n",
    "tmp = train_df.isnull().mean()\n",
    "tmp[tmp > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ce52f",
   "metadata": {},
   "source": [
    "On a 37 valeurs manquantes dans la colonne margin_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pourçentage des valeurs manquantes \n",
    "print('Percent of missing \"margin_low\" records is %.2f%%' %((train_df['margin_low'].isnull().sum()/train_df.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b5266",
   "metadata": {},
   "source": [
    "~2,5% des données dans la colonne margin_low est manquantes. -> voir la distribution de cette variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# la distribution de la variable 'margin_low'\n",
    "ax = train_df[\"margin_low\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\n",
    "train_df[\"margin_low\"].plot(kind='density', color='teal')\n",
    "ax.set(xlabel='margin_low')\n",
    "plt.xlim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ded8fa",
   "metadata": {},
   "source": [
    "### 2.3.1 Imputation des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52788549",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e57f1",
   "metadata": {},
   "source": [
    "On a un DataFrame qui contient X=6 (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df9f384",
   "metadata": {},
   "source": [
    "## Imputation des valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b033ea",
   "metadata": {},
   "source": [
    "## les valeurs manquantes sont de type numerique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466ed47",
   "metadata": {},
   "source": [
    "*** 1. La methode regression lineaire de sklearn ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5448c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les données en deux ensembles : avec et sans valeurs manquantes\n",
    "\n",
    "test= train_df[train_df['margin_low'].isna()] #DataFrame qui contient que des valeurs manquantes dans la colonne margin_low\n",
    "\n",
    "train = train_df[~train_df['margin_low'].isna()] #dataframe sans valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#les variables explicative (X)\n",
    "X_train = train.drop(columns=['margin_low', 'target'])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une instance de StandardScaler\n",
    "scaler_1 = StandardScaler()\n",
    "# Adapter le scaler aux données\n",
    "scaler_1.fit(X_train)\n",
    "\n",
    "# Standardiser les données\n",
    "X_train_scaled = scaler_1.transform(X_train)\n",
    "X_train_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cd8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#la variable cible (y) (target)\n",
    "y_train= train['margin_low']\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd055130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser les modèles\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Naive Model (median)': DummyRegressor(strategy='median'),\n",
    "    'Naive Model (Mean)': DummyRegressor(strategy='mean')\n",
    "}\n",
    "\n",
    "# La validation croisée et les scores RMSE pour chaque modèle\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = (-scores) ** 0.5\n",
    "    results[model_name] = {'mean_rmse': rmse_scores.mean(), 'std_rmse': rmse_scores.std()}\n",
    "\n",
    "# Affichage des résultats\n",
    "for model_name, result in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Mean RMSE: {result['mean_rmse']}\")\n",
    "    print(f\"Standard deviation of RMSE: {result['std_rmse']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee515e",
   "metadata": {},
   "source": [
    "Selon les métriques de RMSE (erreur), le modèle de régression linéaire démontre la meilleure performance parmi les trois modèles testés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model_1: Régression linéaire\n",
    "# Créer et entraîner le modèle de régression linéaire\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation de la variable X_test\n",
    "X_test = test.drop(columns=['margin_low', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff289dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb39e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation des données de X_test\n",
    "X_test_scaled = scaler_1.transform(X_test)\n",
    "X_test_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédire les valeurs manquantes\n",
    "y_predicted = model_1.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54123bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce7e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation d'une copy de notre DataFrame\n",
    "df_reg=train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41974c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacement des valeurs manquantes par les valeurs prédites\n",
    "df_reg.loc[df_reg['margin_low'].isna(), 'margin_low'] = y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ee100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification\n",
    "df_reg.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview adjusted train data\n",
    "df_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb725d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification avec visualisation\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = train_df[\"margin_low\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\n",
    "train_df[\"margin_low\"].plot(kind='density', color='teal')\n",
    "ax = df_reg[\"margin_low\"].hist(bins=15, density=True, stacked=True, color='orange', alpha=0.5)\n",
    "df_reg[\"margin_low\"].plot(kind='density', color='orange')\n",
    "ax.legend(['Raw margin_low', 'Adjusted margin_low'])\n",
    "ax.set(xlabel='margin_low')\n",
    "plt.xlim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681eff60",
   "metadata": {},
   "source": [
    "2. La méthode de régression linéaire de Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et ajuster le modèle aux données d'entraînement\n",
    "model_2 = sm.OLS(y_train, X_train_scaled)\n",
    "model_fit = model_2.fit()\n",
    "\n",
    "# Faire des prédictions sur les données de test mises à l'échelle\n",
    "predicted_values = model_fit.predict(X_test_scaled)\n",
    "\n",
    "# Afficher les valeurs prédites\n",
    "print(predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d9da5",
   "metadata": {},
   "source": [
    "Avec Statsmodels, on doit ajouter manuellement une colonne qui contient une constante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On a besoin d'une constante dans ce maodéle\n",
    "X_train_scaled_with_const = sm.add_constant(X_train_scaled)\n",
    "X_test_scaled_with_const = sm.add_constant(X_test_scaled)\n",
    "X_test_scaled_with_const[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = sm.OLS(y_train, X_train_scaled_with_const).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a779ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour déterminer quelles variables éliminer, nous devons examiner leurs valeurs p.\n",
    "significant_variables = model_2.pvalues[model_2.pvalues < 0.05].index\n",
    "significant_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1a4b3",
   "metadata": {},
   "source": [
    "Toutes les variables sont considérées comme significatives au seuil de valeur p actuel (0,05), donc on doit les conserver toutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f79bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = model_2.predict(X_test_scaled_with_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ed581",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5750c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat=train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat.loc[df_stat['margin_low'].isnull(), 'margin_low'] = predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification avec visualisation\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = train_df[\"margin_low\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\n",
    "train_df[\"margin_low\"].plot(kind='density', color='teal')\n",
    "ax = df_stat[\"margin_low\"].hist(bins=15, density=True, stacked=True, color='orange', alpha=0.5)\n",
    "df_stat[\"margin_low\"].plot(kind='density', color='orange')\n",
    "ax.legend(['Raw margin_low', 'Adjusted margin_low'])\n",
    "ax.set(xlabel='margin_low')\n",
    "plt.xlim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa706c",
   "metadata": {},
   "source": [
    "### Tests statistiques\n",
    "\n",
    "Pour confirmer que la distribution, la moyenne et la variance de la colonne 'margin_low' restent les mêmes après l'imputation des valeurs nulles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f778c4",
   "metadata": {},
   "source": [
    "Distribution test : On peut utiliser un test statistique non paramétrique comme le test de Kolmogorov-Smirnov pour comparer la distribution de la colonne originale avec celle de la colonne remplie.\n",
    "\n",
    "Test de moyenne : On peut effectuer un test t pour comparer les moyennes de la colonne originale et de la colonne remplie. Assurez-vous que vos données répondent aux hypothèses du test t (distribution normale ou échantillon suffisamment grand).\n",
    "\n",
    "Test de variance : On peut utiliser un test de F pour comparer les variances de la colonne originale et de la colonne remplie. Encore une fois, assurez-vous que vos données répondent aux hypothèses du test de F (distribution normale ou échantillon suffisamment grand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"margin_low\"].shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a613a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de distribution\n",
    "def test_distribution(original, filled):\n",
    "    _, p_value = stats.ks_2samp(original, filled)\n",
    "    if p_value < 0.05:\n",
    "        print(p_value, \" -> La distribution n'est pas la même.\")\n",
    "    else:\n",
    "        print(p_value, \" -> La distribution est similaire.\")\n",
    "\n",
    "# Test de moyenne\n",
    "def test_moyenne(original, filled):\n",
    "    _, p_value = stats.ttest_ind(original, filled)\n",
    "    if p_value < 0.05:\n",
    "        print(p_value, \" -> Les moyennes ne sont pas les mêmes.\")\n",
    "    else:\n",
    "        print(p_value, \" -> Les moyennes sont similaires.\")\n",
    "\n",
    "# Test de variance\n",
    "def test_variance(original, filled):\n",
    "    _, p_value = stats.levene(original, filled)\n",
    "    if p_value < 0.05:\n",
    "        print(p_value, \" -> Les variances ne sont pas les mêmes.\")\n",
    "    else:\n",
    "        print(p_value, \" -> Les variances sont similaires.\")\n",
    "\n",
    "# Exécution des tests\n",
    "test_distribution(y_train, df_reg[\"margin_low\"])\n",
    "test_moyenne(y_train, df_reg[\"margin_low\"])\n",
    "test_variance(y_train, df_reg[\"margin_low\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1c7d9",
   "metadata": {},
   "source": [
    "### Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_1.predict(X_train_scaled)\n",
    "# residuals = y_train - y_pred\n",
    "residuals = model_2.resid\n",
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=train_df.margin_low, y=residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9dbae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=residuals, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a1b8c",
   "metadata": {},
   "source": [
    "\n",
    "Le test de Shapiro-Wilk est un test statistique utilisé pour déterminer si un échantillon de données suit une distribution normale (gaussienne). Il est basé sur la comparaison entre les moments observés des données et ceux attendus sous l'hypothèse de normalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de02ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la moyenne et la variance\n",
    "mean = 0\n",
    "variance = np.var(residuals)\n",
    "\n",
    "# Créer une distribution normale\n",
    "normal_dist = stats.norm(loc=0, scale=1)\n",
    "# Créer une distribution gaussienne\n",
    "gaussian_dist = stats.norm(loc=mean, scale=np.sqrt(variance))\n",
    "\n",
    "# Visualiser les données et la distribution gaussienne\n",
    "plt.hist(residuals, bins=30, density=True, alpha=0.5, label='Données observées')\n",
    "x = np.linspace(np.min(residuals), np.max(residuals), 100)\n",
    "plt.plot(x, gaussian_dist.pdf(x), 'r', label='Distribution gaussienne')\n",
    "plt.plot(x, normal_dist.pdf(x), 'g', label='Distribution normale')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Tester l'ajustement\n",
    "kstest_result = stats.kstest(residuals, 'norm', args=(mean, np.sqrt(variance)))\n",
    "shapiro_result = stats.shapiro(residuals)\n",
    "\n",
    "print(\"Test de Kolmogorov-Smirnov:\")\n",
    "print(\"Statistique de test:\", kstest_result.statistic)\n",
    "p_value = kstest_result.pvalue\n",
    "print(\"P-value:\", p_value)\n",
    "if p_value < 0.05:\n",
    "    print(p_value, \" -> Les résidus ne suivent pas une distribution normale\")\n",
    "else:\n",
    "    print(p_value, \" -> Les résidus suivent une distribution normale\")\n",
    "print(\"\\nTest de Shapiro-Wilk:\")\n",
    "print(\"Statistique de test:\", shapiro_result.statistic)\n",
    "p_value = shapiro_result.pvalue\n",
    "print(\"P-value:\", p_value)\n",
    "if p_value < 0.05:\n",
    "    print(p_value, \" -> Les résidus ne suivent pas une distribution normale\")\n",
    "else:\n",
    "    print(p_value, \" -> Les résidus suivent une distribution normale\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69d87e",
   "metadata": {},
   "source": [
    "Pour évaluer l'homoscédasticité des résidus, nous utilisons le test statistique de Breusch-Pagan. Ce test compare la variance des résidus avec une ou plusieurs variables indépendantes pour déterminer s'ils présentent une homoscédasticité. En d'autres termes, il examine l'uniformité de la dispersion des résidus sur toute la plage des valeurs prises par les variables indépendantes. <br>L'hétéroscédasticité, qui est le contraire de l'homoscédasticité, se manifeste lorsque la variance des résidus varie de manière non constante à travers les valeurs prédites de la variable dépendante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c713231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de Breusch-Pagan\n",
    "test_bp = het_breuschpagan(residuals, X_train_scaled_with_const)\n",
    "print(\"Test de Breusch-Pagan:\")\n",
    "print(\"Statistique de test:\", test_bp[0])\n",
    "p_value = test_bp[1]\n",
    "print(\"P-value:\", p_value)\n",
    "if p_value < 0.05:\n",
    "    print(p_value, \" -> Les résidus présentent une hétéroscédasticité\")\n",
    "else:\n",
    "    print(p_value, \" -> Les résidus présentent une homoscédasticité\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098a641",
   "metadata": {},
   "source": [
    "La multicollinéarité: C'est une condition dans laquelle deux ou plus de deux variables indépendantes (ou prédictives) dans un modèle de régression sont fortement corrélées entre elles. En d'autres termes, il existe une relation linéaire élevée entre au moins deux des variables indépendantes.<br>plusieurs méthodes pour vérifier la multicollinéarité dans un modèle de régression,parmi les quelles:<br>\n",
    "Variance Inflation Factor (VIF) : Le VIF mesure l'importance de la multicollinéarité dans une régression. Un VIF élevé (généralement supérieur à 10) indique une multicollinéarité problématique.<br>Les variable avec un vif relativement faible contribue de manière unique à expliquer la variation dans la variable dépendante, ce qui est souhaitable dans de nombreux modèles statistiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_scaled, i) for i in range(X_train_scaled.shape[1])]\n",
    "print(vif_data, '\\n')\n",
    "\n",
    "seuil = 10 # Un seuil de 10 est couramment utilisé\n",
    "for index, row in vif_data.iterrows():\n",
    "    feature = row['feature']\n",
    "    vif = row['VIF']\n",
    "    if vif > seuil:\n",
    "        print(f\"La variable '{feature}' a un VIF élevé de {vif:.2f}, ce qui indique une forte corrélation avec d'autres variables explicatives.\")\n",
    "    else:\n",
    "        print(f\"La variable '{feature}' a un VIF de {vif:.2f}, ce qui indique qu'elle est indépendante des autres variables explicatives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff8d33d",
   "metadata": {},
   "source": [
    "***Exploration des données***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_reg, hue='target', corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a09738",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_reg.corr()\n",
    "masque=np.triu(correlation_matrix)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", mask=masque, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e7be7",
   "metadata": {},
   "source": [
    "la target est tres correlée avec la longeur des billets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1764bc5",
   "metadata": {},
   "source": [
    "## 2.3.2 Detection des outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad24542",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plt.subplot(321)\n",
    "sns.boxplot(data=df_reg, x='diagonal')\n",
    "\n",
    "plt.subplot(322)\n",
    "sns.boxplot(data=df_reg, x='height_left')\n",
    "\n",
    "plt.subplot(323)\n",
    "sns.boxplot(data=df_reg, x='height_right')\n",
    "\n",
    "plt.subplot(324)\n",
    "sns.boxplot(data=df_reg, x='margin_low')\n",
    "\n",
    "plt.subplot(325)\n",
    "sns.boxplot(data=df_reg, x='margin_up')\n",
    "\n",
    "plt.subplot(326)\n",
    "sns.boxplot(data=df_reg, x='length')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ec04b",
   "metadata": {},
   "source": [
    "Les boxplots révèlent la présence d'outliers dans plusieurs features. <br> Cela suggère que ces valeurs pourraient être liées à des faux billets, caractérisés par des dimensions non conformes, ou bien à des billets de grande valeur, comme les billets de 500€, ou de petites valeurs comme celles de 5€. <br> Ainsi, il est prévu de conserver les outliers afin de poursuivre l'analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b2f93",
   "metadata": {},
   "source": [
    "## 2.3.1.1 Data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da073f7c",
   "metadata": {},
   "source": [
    "## 2.4 Preparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245bda3",
   "metadata": {},
   "source": [
    "## Choix du modéle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38693e62",
   "metadata": {},
   "source": [
    "Type de données categorielles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0f1af",
   "metadata": {},
   "source": [
    "2.5 Modelisation avec split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbace0",
   "metadata": {},
   "source": [
    "2.5.1 Dummy clussifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_reg.drop(columns='target')\n",
    "y=df_reg['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0618ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split (X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c78927",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_1= DummyClassifier(strategy=\"most_frequent\")\n",
    "estimator_1.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f246fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = estimator_1.predict(test_X)\n",
    "y_pred_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5290ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_pred_1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef019f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cd266",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e10c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcdcc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_score= estimator_1.score(train_X, train_y)\n",
    "te_score= estimator_1.score(test_X, test_y)\n",
    "print(f\"le score de train est {tr_score}, et de test est {te_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = estimator_1.predict(test_X)\n",
    "y_pred_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1461602",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed29d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - abs(y_pred_1 - test_y.values).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ffb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat=confusion_matrix(test_y, y_pred_1, labels=estimator_1.classes_)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=mat, display_labels= estimator_1.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22e0df",
   "metadata": {},
   "source": [
    "## 2. Regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e02b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_2 = LogisticRegression(solver='liblinear') #small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_2.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be4925",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2=estimator_2.predict(test_X)\n",
    "y_pred_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = estimator_2.predict_proba(test_X).round(2) #La probabilité de l'appartenance à tel ou tel classe\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb31731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conf=confusion_matrix(test_y, y_pred_2, labels=estimator_2.classes_)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf, display_labels= estimator_2.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da523c0",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_3 = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734b053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_3.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_3=estimator_3.predict(test_X)\n",
    "y_pred_3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71119699",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=confusion_matrix(test_y, y_pred_3, labels=estimator_3.classes_)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf, display_labels= estimator_3.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf327590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8de422c9",
   "metadata": {},
   "source": [
    "Modelisation avec cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_reg.drop(columns=['target'])\n",
    "y=df_reg['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e7b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une instance de StandardScaler\n",
    "scaler_2 = StandardScaler()\n",
    "# Adapter le scaler aux données\n",
    "scaler_2.fit(X)\n",
    "\n",
    "# Standardiser les données\n",
    "X_scaled = scaler_2.transform(X)\n",
    "X_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729bf71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choix du modele\n",
    "Logistic_Regression = LogisticRegression()\n",
    "svc = SVC()\n",
    "Majority_Model = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# Initialiser les modèles\n",
    "models = {\n",
    "    'Logistic Regression': Logistic_Regression,\n",
    "    'SVC': svc,\n",
    "    'Majority Model': Majority_Model\n",
    "}\n",
    "\n",
    "# La validation croisée et les scores de précision pour chaque modèle\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "    results[model_name] = {'mean_accuracy': scores.mean(), 'std_accuracy': scores.std()}\n",
    "\n",
    "# Affichage des résultats\n",
    "for model_name, result in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Mean Accuracy: {result['mean_accuracy']}\")\n",
    "    print(f\"Standard deviation of Accuracy: {result['std_accuracy']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dfd0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation de la performance du modèle\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 6), sharey=True)\n",
    "\n",
    "common_params = {\n",
    "    \"X\": X_scaled,\n",
    "    \"y\": y,\n",
    "    \"train_sizes\": np.linspace(0.1, 1.0, 5),\n",
    "    \"cv\": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0),\n",
    "    \"score_type\": \"both\",\n",
    "    \"n_jobs\": 4,\n",
    "    \"line_kw\": {\"marker\": \"o\"},\n",
    "    \"std_display_style\": \"fill_between\",\n",
    "    \"score_name\": \"Accuracy\",\n",
    "}\n",
    "\n",
    "for ax_idx, estimator in enumerate([Logistic_Regression, svc, Majority_Model]):\n",
    "    LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax[ax_idx])\n",
    "    handles, label = ax[ax_idx].get_legend_handles_labels()\n",
    "    ax[ax_idx].legend(handles[:2], [\"Training Score\", \"Test Score\"])\n",
    "    ax[ax_idx].set_title(f\"Learning Curve for {estimator.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05ff09",
   "metadata": {},
   "source": [
    "En observant les courbes d'apprentissage, on peut déterminer que le modèle DummyClassifier montre un sous-apprentissage (underfitting) tandis que les deux modèles Logistic Regression et SVC sont comparables et montrent de bons résultats. <br> On a choisi d'optimiser le modele de Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimisation des hyperparamètres\n",
    "\n",
    "# Définir la grille des hyperparamètres à rechercher\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Initialiser LogisticRegression\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Créer l'objet GridSearchCV\n",
    "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5, scoring='accuracy', n_jobs=4, verbose=1, return_train_score=True)\n",
    "\n",
    "# Exécuter la recherche sur grille sur les données d'entraînement\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres et le score associé\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultize(grid_search) : \n",
    "    \"\"\"build a DataFrame from the results of a GridSearchCV\"\"\"\n",
    "\n",
    "    res = pd.DataFrame(grid_search.cv_results_)\n",
    "    cols = [i for i in res.columns if 'split' not in i]\n",
    "    res = res[cols]\n",
    "    res = res.round(2).iloc[:, 4:].sort_values('mean_test_score', ascending=False)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultize(grid_search).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choisir le bon modele\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrice du confusion\n",
    "y_pred = best_model.predict(X_scaled)\n",
    "mat=confusion_matrix(y, y_pred)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b38640",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"y_true\" : y, \"y_pred\" : y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(mat, annot=True, cmap='coolwarm', fmt='d', cbar=False, vmax=1000, vmin=-1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50349ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=mat)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c316c23",
   "metadata": {},
   "source": [
    "On voit que nous avons 10 faux négatifs (10 faux billets classés comme vrais) et 4 faux positifs (4 vrais billets classés comme faux). <br> Nous allons changer la métrique de scoring afin d'avoir moins de faux négatifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc55aac6",
   "metadata": {},
   "source": [
    "### Choix metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b650a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sco in ['accuracy', 'recall', 'f1']: # , 'roc_auc', 'average_precision'\n",
    "    print(sco)\n",
    "    # Définir la grille des hyperparamètres à rechercher\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "\n",
    "    # Initialiser LogisticRegression\n",
    "    logistic_regression = LogisticRegression()\n",
    "\n",
    "    # Créer l'objet GridSearchCV\n",
    "    grid_search = GridSearchCV(logistic_regression, param_grid, cv=5, scoring=sco)\n",
    "\n",
    "    # Exécuter la recherche sur grille sur les données d'entraînement\n",
    "    grid_search.fit(X_scaled, y)\n",
    "\n",
    "    # Afficher les meilleurs hyperparamètres et le score associé\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    #matrice de confusion\n",
    "    y_pred = best_model.predict(X_scaled)\n",
    "    mat=confusion_matrix(y, y_pred)\n",
    "    print(mat)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=mat)\n",
    "    disp.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcaab0f",
   "metadata": {},
   "source": [
    "on choisit la metric f1 qui a le meilleur compromis et le moins de faux négatifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b1a0b",
   "metadata": {},
   "source": [
    "### Choix du seuil de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la grille des hyperparamètres à rechercher\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Initialiser LogisticRegression\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Créer l'objet GridSearchCV\n",
    "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Exécuter la recherche sur grille sur les données d'entraînement\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres et le score associé\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "#matrice de confusion\n",
    "y_pred = best_model.predict(X_scaled)\n",
    "mat=confusion_matrix(y, y_pred)\n",
    "print(mat)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=mat)\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = best_model.predict_proba(X_scaled)\n",
    "nouveau_seuil = 0.25\n",
    "y_pred = (y_proba[:, 1] > nouveau_seuil).astype(int) #Faux billet si la probabilité est supérieure au seuil\n",
    "mat=confusion_matrix(y, y_pred)\n",
    "print(mat)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=mat)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95168f",
   "metadata": {},
   "source": [
    "0.25 est le seuil qui a le meilleur compromis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7423a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
